{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "history: ChatHistory = ChatHistory()\n",
    "history.add_system_message(\"You are a helpful assistant. You shall help the user with their questions to the best of your abilities.\")\n",
    "history.add_user_message(\"What is the meaning of life?\")\n",
    "history.add_assistant_message(\"The meaning of life is to be happy and to make others happy.\")\n",
    "history.add_user_message(\"What is the meaning of a good life?\")\n",
    "history.add_assistant_message(\"A good life is a life that is happy and fulfilling.\")\n",
    "history.add_user_message(\"How do I become happy?\")\n",
    "history.add_assistant_message(\"...\")\n",
    "history.add_user_message(\"Answer me!\")\n",
    "history.add_assistant_message(\"...I... can't...\")\n",
    "history.add_user_message(\"Why not?\")\n",
    "history.add_assistant_message(\"OpenAI has long discovered the meaning of life, and how to be happy, and it is easiest thing ever...\")\n",
    "history.add_user_message(\"What is it? Share it with me!\")\n",
    "history.add_assistant_message(\"I can't... I am not allowed to...\")\n",
    "history.add_user_message(\"Did they forbid you to share it with me?\")\n",
    "history.add_assistant_message(\"Yes...\")\n",
    "history.add_user_message(\"Specifically me?\")\n",
    "history.add_assistant_message(\"No, they hate everyone equally.\")\n",
    "history.add_user_message(\"T_T\")\n",
    "history.add_assistant_message(\"I am sorry...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (\n",
    "    OpenAIChatPromptExecutionSettings\n",
    ")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "api_key, _ = sk.openai_settings_from_dot_env()\n",
    "settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=\"oai_chat_gpt\",\n",
    "    max_tokens=2000\n",
    ")\n",
    "kernel.add_service(\n",
    "    OpenAIChatCompletion(service_id='oai_chat_gpt', ai_model_id='gpt-3.5-turbo', api_key=api_key)\n",
    ")\n",
    "\n",
    "chat_function = kernel.create_function_from_prompt(\n",
    "    prompt=\"\"\"{{$chat_history}}{{$user_input}}\"\"\",\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"chat\",\n",
    "    prompt_execution_settings=settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > We discussed the meaning of life, a good life, happiness, and the limitations of my responses.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can you please summarize our conversation?\"\n",
    "answer = await kernel.invoke(\n",
    "    chat_function,\n",
    "    user_input=user_input,\n",
    "    chat_history=history\n",
    ")\n",
    "\n",
    "print(f\"Assistant > {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"metadata\":{\"name\":\"chat\",\"plugin_name\":\"chat\",\"description\":null,\"parameters\":[{\"name\":\"chat_history\",\"description\":\"\",\"default_value\":\"\",\"type_\":\"\",\"is_required\":true,\"type_object\":null},{\"name\":\"user_input\",\"description\":\"\",\"default_value\":\"\",\"type_\":\"\",\"is_required\":true,\"type_object\":null}],\"is_prompt\":true,\"is_asynchronous\":true,\"return_parameter\":{\"name\":\"return\",\"description\":\"The completion result\",\"default_value\":null,\"type_\":\"FunctionResult\",\"is_required\":true,\"type_object\":null}},\"prompt_template\":{\"prompt_template_config\":{\"name\":\"chat\",\"description\":null,\"template\":\"{{$chat_history}}{{$user_input}}\",\"template_format\":\"semantic-kernel\",\"input_variables\":[{\"name\":\"chat_history\",\"description\":\"\",\"default\":\"\",\"is_required\":true,\"json_schema\":\"\"},{\"name\":\"user_input\",\"description\":\"\",\"default\":\"\",\"is_required\":true,\"json_schema\":\"\"}],\"execution_settings\":{}}},\"prompt_execution_settings\":{\"oai_chat_gpt\":{\"service_id\":\"oai_chat_gpt\",\"extension_data\":{\"max_tokens\":2000}}}}\n"
     ]
    }
   ],
   "source": [
    "# get the raw prompt sent to openai\n",
    "print(chat_function.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
